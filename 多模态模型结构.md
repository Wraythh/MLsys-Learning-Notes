# 多模态
  最近搞多模态RLHF，经常会用到vllm推理多模态模型，这其中的很多概念和原理都是第一次接触，这篇文章简单把学习过程记录和总结一下。想弄明白一条多模态数据从预处理到最终推理都经过了什么。
## 数据的读取
## 多模态embedding
  多模态模型的整体架构有很多不同的种类，目前只了解统一解码器架构，这种架构和大语言模型可以很好的结合在一起。
  首先回顾一下文本数据的处理，是比较简单的，一句文本经过分词器(tokenizer)分词后，经过embedding层，就会得到每个token_id对应的embedding向量。这个embedding，本质上是一个线性层，将input_ids 转换为 one_hot 后，对其做一个线性操作，变为[seq, hidden_size]的tensor。
  写成伪代码就是下面的形式
```
text: str = "who are you?"
# size of token_ids is [seq,]
token_ids: List[int] = tokenizer(text)
# size of one_hot_token_ids is [seq, vocab_size]
one_hot_token_ids = one_hot(token_ids)
# size of text_embeddings is [seq, hidden_size]
text_embeddings: torch.Tensor = input_embedding(one_hot_token_ids)
```
于是我们就得到了文本数据的embedding，可以丢给后面的transformer decoder layer进行计算。
在多模态场景下，会变得复杂，我们需要把 image 也变成 embedding 的格式并且跟文本产生的embedding具有相同的hidden_size然后插入文本的embedding中，才能丢给后面的decoder layer处理。
下面我们看一下如何对图片进行操作。
### 图片和文本预处理
我们得到的图片数据应该是一个RGB格式的具有长宽两个维度的数据，通常RGB格式这个维度叫做图像的channel，于是每张图片的size就是[channel, width, height]。
在预处理阶段，要保证两件事情，一是让所有的图片大小都在一个范围内，防止图片过大过小，比如处理的最大分辨率是2048，最小分辨率是512。其次，保证所有的图片都是RGB格式。这里是看了verl中的图片的预处理逻辑，不一定代表所有场景。
如果图片过大，就会把图片裁剪到合理的大小
使用PIL库的image.resize操作，可以将图片按比例缩放到指定的大小。在通过image.convert('RGB')转为RGB格式
代码如下
```
image = Image.open(BytesIO(image['bytes']))
image = image.resize((width, height))
image = image.convert('RGB')
```
这样我们就使用PIL库对图片做了相应的预处理。
然后把图片存入 row_dict['multi_modal_data'] 字段丢给vllm
```
row_dict['multi_modal_data'] = {'image': image}
```
文本方面

### 图片编码器
### 线性适配器

### one-step-async-rl
基于verl做了一版 one step async rl 的方案，在这里简单总结一下。
最早是看到verl-pipeline的项目https://github.com/agentica-project/verl-pipeline使用了这种方法，评估了一下觉得这种方案主要有两个优势。
- 一是算力利用更充分，尤其是资源比较多的情况。
  对于推理侧，通常情况下不会打满gpu，一个vllm实例上只有1-2条数据，这时候如果把GPU减半，实际上吞吐会增加很多，导致时间并不会慢多少。
  对于训练侧，减少了卡数相当于增加梯度累计的数量，减少了通信占比，也会增大吞吐。
- 二是扩展性更强，同样也是资源比较多的情况。
  大部分时候限制扩展性的是batch_size。由于想尽可能的on-policy，通常不会把采样的batch_size加的太大，train的batch_size又会导致掉点，所以想增加扩展性不容易。
  那么训推分离的方案会使得扩展性比较好，至少增加一半的卡数，把本来的训练和推理放在两组卡上，就会有不少的提升效果。如果能保证训练和推理的时间差不多长，那么这里的加速比基本可以达到100%。

所以我们就把这个方案落地到了实际的训练中，然而在落地过程中遇到了很多困难。
一是off_policy其实还是有一定影响的。很多论文都说没什么影响，包括verl官方实现的one_step_async_rl recipe https://github.com/volcengine/verl/tree/main/recipe/one_step_off_policy也说没什么影响。
但是实际上说没什么影响的跑的步数都比较少，或者实验比较偏demo，经过我们的实测，有几个指标实际上是不能完全对齐的，例如entropy和kl_loss。这其实会导致评测指标掉点，虽然最终显示通过加数据也能追回来，
但是说明off_policy就是会有一定影响的。并且会随着实验的进行逐渐扩大。最终跟算法协调落地的时候，我们也是比的 time/performance 曲线，而不是 steps/performance。
这也引申出对于infra工作的一些思考。大部分时候infra做的优化其实都会有一定程度的影响，只是这个影响最终能不能通过加数据来掩盖，如果有scaling的趋势，那么加数据是完全可行的，那最终就是比 time/performance。
但如果影响最终会导致上限变低，那么算法同学肯定会不买账，好在大部分情况加数据都是很有效的策略，所以我们的一些性能优化工作才能得以推进。

接下来讲一下对于RL的一些思考：
最近看了李沐老师的params server的工作，不禁感慨，太阳下没有新鲜事，当时为了性能的提升，学术界就在研究async的更新机制，对深度学习系统的影响，本质上和现在的async rl系统非常类似。
在param server中，参数更新和梯度计算的强依赖被打破，而在async rl中，用来采样的权重和训练得到的权重间的强依赖被打破，导致了off-policy的问题。那么想要回答RL系统终究会发展成什么样，如果只是看params server和训练框架的
发展趋势，能不能得到可靠的结论呢？我觉得答案是大部分可以，因为异步这种机制对于算法同学来讲天然的不擅长，这东西是违背人脑思考逻辑的，而训练这个场景，主要的用户就是算法同学，所以，用户不变，那么需求就不太会有本质的变化，所以async rl这个东西也可能只是昙花一现。由于很多做rl系统的人都是做serving出身，serving天然的需要高并发和很大的吞吐，这就很需要async，但serving其实不是算法同学关心的事情，算法更关注怎么训练出来一个好用的模型，
serving是交给工程同事来做的，所以异步并没有得到算法同学的青睐。那么如果async不是趋势，强化学习最终又会收敛到什么样的框架呢？
首先比较明确的一点是，rl大概率不会像预训练一样收敛到一个标准的一个工程化流程。

# 多模态
  最近搞多模态RLHF，经常会用到vllm推理多模态模型，这其中的很多概念和原理都是第一次接触，这篇文章简单把学习过程记录和总结一下。想弄明白一条多模态数据从预处理到最终推理都经过了什么。这里以siglip和llava的实现为例进行介绍，其他的模型暂时不太了解，以后再补充。
## 数据的读取
## 多模态embedding
  多模态模型的整体架构有很多不同的种类，目前只了解统一解码器架构，这种架构和大语言模型可以很好的结合在一起。
  首先回顾一下文本数据的处理，是比较简单的，一句文本经过分词器(tokenizer)分词后，经过embedding层，就会得到每个token_id对应的embedding向量。这个embedding，本质上是一个线性层，将input_ids 转换为 one_hot 后，对其做一个线性操作，变为[seq, hidden_size]的tensor。
  写成伪代码就是下面的形式
```
text: str = "who are you?"

# size of token_ids is [seq,]
token_ids: List[int] = tokenizer(text)

# size of one_hot_token_ids is [seq, vocab_size]
one_hot_token_ids = one_hot(token_ids)

# size of text_embeddings is [seq, hidden_size]
text_embeddings: torch.Tensor = input_embedding(one_hot_token_ids)
```
于是我们就得到了文本数据的embedding，可以丢给后面的transformer decoder layer进行计算。
在多模态场景下，会变得复杂，我们需要把 image 也变成 embedding 的格式并且跟文本产生的embedding具有相同的hidden_size然后插入文本的embedding中，才能丢给后面的decoder layer处理。
下面我们看一下如何对图片进行操作。
### 图片和文本预处理
图片和文本都需要相应的预处理才能丢给模型
我们得到的图片数据应该是一个RGB格式的具有长宽两个维度的数据，通常RGB格式这个维度叫做图像的channel，于是每张图片的size就是[channel, width, height]。
在预处理阶段，要保证两件事情，一是让所有的图片大小都在一个范围内，防止图片过大过小，比如处理的最大分辨率是2048，最小分辨率是512。其次，保证所有的图片都是RGB格式。这里是看了verl中的图片的预处理逻辑，不一定代表所有场景。
如果图片过大，就会把图片裁剪到合理的大小
使用PIL库的image.resize操作，可以将图片按比例缩放到指定的大小。在通过image.convert('RGB')转为RGB格式
代码如下
```
image = Image.open(BytesIO(image['bytes']))
image = image.resize((width, height))
image = image.convert('RGB')
```
这样我们就使用PIL库对图片做了相应的预处理。
文本方面
文本需要做一个重要的预处理，就是给图片预留位置。很多模型都是采用添加额外的字段来实现给图片数据预留位置，例如<image>字段，在text中加入相应的字段，例如
```
[|Human|]: <image> what is written in the image? [|AI|]:
```
就表示需要把图片插入在<image>所在的位置，这个插入是在embedding层面的插入，即把image的embedding放在text_embedding预留的<image>对应的embedding位置上。这个后续再说，现在先继续讲怎么把图片丢给image encoder；来得到相应的embeddings，这里需要经过三个阶段
+ 一是对图片做resize, padding, patching等操作将图片分成一个一个固定size的patch。
+ 二是将这些patch丢给image_encoder获得图像对应的feature。
+ 三是把图像feature经过一个线性投影，投影到跟文本的hidden_size具有相同的维度。
### 图片patch
编码器通常采用一个已经训练好的Encoder，例如CLIP，SIGLIP等，这里着重看一下LlavaNext是如何处理图片得到embedding的
加入我们有一个[27,104]的图片，有RGB三个channel，于是图片的size是[3,27,104]，
首先LlavaNext会根据图片的大小选择一个最合适的分辨率，这个例子中会选择[384,384]。然后会把image经过resize和padding操作，使图片变为[3,384,384]的大小，接下来会对图片进行切分patch的操作，由于llava动态分辨率的特殊性，这里切分patch会按照固定的分辨率进行切分，一个patch的大小为[384,384]，所以这个图片最终只有一个patch。llava 还会把原图resize到[384,384]一起丢给后面的大语言模型。
经过patch和原图的拼接之后，会得到一个[2,3,384,384]的pixel_value的tensor，到这里为止，就完成了图片的预处理阶段。
这部分的伪代码如下：
```
image = np.array([3,27,104])
resolution_height, resolution_width = choose_best_resolution(image)

# image size [3, 384, 384]
image = resize_and_padding(image, [3, resolution_height, resolution_width])

# pixel_values size [2, 3, 384, 384]
pixel_values = patch(image)
```
### 图片编码器 Image Encoder
获得pixel_value之后，接下来就把这个pixel_value丢给siglip的vision_model来获得图像的feature
siglip的vision_model是一个叫vision_tower的东西
### 线性投影 MultiModalProjector
线性投影的代码如下
```
hidden_states = self.downsampleblock(image_features)
hidden_states = self.layernorm(hidden_states)
hidden_states, _ = self.linear_1(hidden_states)
hidden_states = self.activation(hidden_states)
hidden_states, _ = self.linear_2(hidden_states)
```
这里比较简单，有些模型会做下采样块，为什么要做下采样呢？
